{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics - Programming Task 1 {-}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Due Date**: Friday Week 6 (4 April 2025) @ 11.59pm  \n",
    "**Submission**: Provide your answers in this Jupypter notebook and submit it via iLearn link  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predictive Analysis of Credit Card Defaults**\n",
    "\n",
    "### Objective: \n",
    "- This assignment focuses on a dataset of customers' default payments.\n",
    "- The primary goal is to predict which credit card clients are likely to default using various data mining methods.\n",
    "\n",
    "### Background: \n",
    "Traditional risk management models classify clients as either credible or not credible based on their likelihood of default. This project aims to refine this classification by identifying specific individuals who are likely to default, enhancing the precision of credit risk assessments.\n",
    "\n",
    "Target variable\n",
    "- default.payment.next.month: Default payment (1=yes, 0=no)\n",
    "\n",
    "The dataset contains the following features\n",
    "\n",
    "1. ID: ID of each client\n",
    "2. LIMIT_BAL: Amount of given credit in dollars (includes individual and family/supplementary credit\n",
    "3. SEX: Gender (1=male, 2=female)\n",
    "4. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "5. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "6. AGE: Age in years\n",
    "7. PAY_0: Repayment status in September (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "8. PAY_2: Repayment status in August (scale same as above)\n",
    "9. PAY_3: Repayment status in July, (scale same as above)\n",
    "10. PAY_4: Repayment status in June (scale same as above)\n",
    "11. PAY_5: Repayment status in May (scale same as above)\n",
    "12. PAY_6: Repayment status in April (scale same as above)\n",
    "13. BILL_AMT1: Amount of bill statement in September (dollars)  \n",
    "14. BILL_AMT2: Amount of bill statement in August (dollars)  \n",
    "15. BILL_AMT3: Amount of bill statement in July (dollars)  \n",
    "16. BILL_AMT4: Amount of bill statement in June (dollars)  \n",
    "17. BILL_AMT5: Amount of bill statement in May (dollars)  \n",
    "18. BILL_AMT6: Amount of bill statement in April (dollars)   \n",
    "19. PAY_AMT1: Amount of previous payment in September (dollars)  \n",
    "20. PAY_AMT2: Amount of previous payment in August (dollars)  \n",
    "21. PAY_AMT3: Amount of previous payment in July (dollars)   \n",
    "22. PAY_AMT4: Amount of previous payment in June (dollars)  \n",
    "23. PAY_AMT5: Amount of previous payment in May (dollars)   \n",
    "24. PAY_AMT6: Amount of previous payment in April (dollars)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1** - Reading the dataset (Total Marks: 20)\n",
    "\n",
    "\n",
    "\n",
    "**Q1**. Create a pandas dataframe contining the first 10,000 rows from the credit card dataset provided in the **assignment_data** folder \n",
    "- Delete 'ID' column\n",
    "- Print `info()` of the dataframe \n",
    "\n",
    "(5 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   SEX                         9617 non-null   float64\n",
      " 2   EDUCATION                   9617 non-null   float64\n",
      " 3   MARRIAGE                    9617 non-null   float64\n",
      " 4   AGE                         9617 non-null   float64\n",
      " 5   PAY_0                       9617 non-null   float64\n",
      " 6   PAY_2                       9617 non-null   float64\n",
      " 7   PAY_3                       9617 non-null   float64\n",
      " 8   PAY_4                       9617 non-null   float64\n",
      " 9   PAY_5                       9617 non-null   float64\n",
      " 10  PAY_6                       9642 non-null   float64\n",
      " 11  BILL_AMT1                   9642 non-null   float64\n",
      " 12  BILL_AMT2                   9642 non-null   float64\n",
      " 13  BILL_AMT3                   9642 non-null   float64\n",
      " 14  BILL_AMT4                   9642 non-null   float64\n",
      " 15  BILL_AMT5                   9632 non-null   float64\n",
      " 16  BILL_AMT6                   9632 non-null   float64\n",
      " 17  PAY_AMT1                    9632 non-null   float64\n",
      " 18  PAY_AMT2                    9632 non-null   float64\n",
      " 19  PAY_AMT3                    9990 non-null   float64\n",
      " 20  PAY_AMT4                    9990 non-null   float64\n",
      " 21  PAY_AMT5                    9710 non-null   float64\n",
      " 22  PAY_AMT6                    9710 non-null   float64\n",
      " 23  default payment next month  10000 non-null  int64  \n",
      "dtypes: float64(22), int64(2)\n",
      "memory usage: 1.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with the first 10,000 rows.\n",
    "df = pd.read_excel('assignment_data/credit_data.xlsx', nrows=10000)\n",
    "\n",
    "# Drop the 'ID' column.\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Print the info of the DataFrame to inspect its structure.\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. List which **features** are *numeric*, *ordinal*, and *nominal* variables, and how many features of each kind there are in the dataset.\n",
    "To answer this question \n",
    "\n",
    "- Find the definitions of numeric, ordinal and nominal variables in the course material    \n",
    "- Carefully consider what values each feature can take as well as the output of `df.info()`. \n",
    "\n",
    "Your answer should be written up in Markdown and include:\n",
    "1) A table listing all the features present in the dataset and their type (fill out the table template provided below) and\n",
    "2) A brief description of the contents of the table.\n",
    "\n",
    "|Variable Kind|Number of Features|Feature Names\n",
    "| --- | --- | --- |\n",
    "| Numeric | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable Kind | Number of Features | Feature Names |\n",
    "|---------------|--------------------|----------------|\n",
    "| Numeric       | 14                 | LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6 |\n",
    "| Ordinal       | 7                  | EDUCATION, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6 |\n",
    "| Nominal       | 3                  | SEX, MARRIAGE, default payment next month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Description:**\n",
    "\n",
    "**Numeric Variables:**\n",
    "These features represent measurable quantities (e.g., credit limit, age, bill amounts, payment amounts) that can be used in mathematical operations such as addition, subtraction, and averaging.\n",
    "\n",
    "**Ordinal Variables:**\n",
    "These variables (e.g., education levels and repayment statuses) have a clear ranking or sequence, even though the exact distance between categories may not be consistent. For instance, different levels of education imply an ordered progression, and higher PAY_x values indicate longer payment delays.\n",
    "\n",
    "**Nominal Variables:**\n",
    "These are categorical variables without an inherent order. In this dataset, SEX, MARRIAGE, and default.payment.next.month fall into this category, as they simply distinguish between groups (e.g., male/female, married/single, default/non-default) without any ranked relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3.** Missing Values. \n",
    "\n",
    "- Print out the number of missing values for each variable in the dataset and comment on your findings.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_BAL                       0\n",
      "SEX                           383\n",
      "EDUCATION                     383\n",
      "MARRIAGE                      383\n",
      "AGE                           383\n",
      "PAY_0                         383\n",
      "PAY_2                         383\n",
      "PAY_3                         383\n",
      "PAY_4                         383\n",
      "PAY_5                         383\n",
      "PAY_6                         358\n",
      "BILL_AMT1                     358\n",
      "BILL_AMT2                     358\n",
      "BILL_AMT3                     358\n",
      "BILL_AMT4                     358\n",
      "BILL_AMT5                     368\n",
      "BILL_AMT6                     368\n",
      "PAY_AMT1                      368\n",
      "PAY_AMT2                      368\n",
      "PAY_AMT3                       10\n",
      "PAY_AMT4                       10\n",
      "PAY_AMT5                      290\n",
      "PAY_AMT6                      290\n",
      "default payment next month      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Check for missing values in each column of the dataframe\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "- **Complete Variables:** LIMIT_BAL and the target variable (default payment next month) have no missing values, indicating complete data availability.\n",
    "\n",
    "- **Moderate Missingness:** Several features such as SEX, EDUCATION, MARRIAGE, AGE, and most of the repayment status (PAY_0 to PAY_6) and billing amount (BILL_AMT1 to BILL_AMT6) variables have a noticeable but still manageable amount of missing values.\n",
    "\n",
    "- **Low Missingness:** PAY_AMT3 and PAY_AMT4 have very few missing entries compared to other variables, indicating higher completeness.\n",
    "\n",
    "Overall, while the dataset is mostly complete, addressing the existing missing values through appropriate methods (e.g., imputation or removal) will be essential to maintain data quality and ensure reliable analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features (Total Marks: 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** \n",
    "\n",
    "- Use an appropriate `pandas` function to impute missing values using one of the following two strategies: `mean` and `mode`. (10 marks)\n",
    "- Take into consideration the type of each variable (as in Q2 above) and the best practices we discussed in class/lecture notes\n",
    "- Explain what data imputation is, how you have done it here, and what decisions you had to make. (5 marks)\n",
    "\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "import pandas as pd\n",
    "\n",
    "# Define features based on the classification:\n",
    "numeric_features = [\n",
    "    'LIMIT_BAL', 'AGE',\n",
    "    'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "    'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'\n",
    "]\n",
    "\n",
    "ordinal_features = ['EDUCATION', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "nominal_features = ['SEX', 'MARRIAGE', 'default payment next month']\n",
    "\n",
    "# Impute missing values in numeric columns using the mean\n",
    "df[numeric_features] = df[numeric_features].fillna(df[numeric_features].mean())\n",
    "\n",
    "# Impute missing values in ordinal and nominal columns using the mode\n",
    "for col in ordinal_features + nominal_features:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Data Imputation Explanation:**\n",
    "\n",
    "Data imputation is the process of replacing missing values in a dataset with estimated values so that the dataset becomes complete and can be used for further analysis or modeling. This is important because missing data can lead to biased results or errors when training predictive models.\n",
    "\n",
    "**How It Was Done Here:**\n",
    "\n",
    "- **Numeric Variables:**\n",
    "For continuous variables (LIMIT_BAL, AGE, bill amounts, and payment amounts), the missing values were replaced using the mean of the available data. This is appropriate because the mean provides a representative value for variables that can be added, subtracted, or averaged.\n",
    "\n",
    "- **Ordinal and Nominal Variables:**\n",
    "For categorical variables, both ordinal (EDUCATION and repayment status features such as PAY_0 to PAY_6) and nominal (SEX, MARRIAGE, and default payment next month), the missing values were replaced using the mode (the most frequent value). This decision is based on the fact that for categorical data, computing an average is not meaningful, and using the mode maintains the most common category.\n",
    "\n",
    "**Decisions Made:**\n",
    "\n",
    "- **Method Choice:**\n",
    "The decision to use mean for numeric variables and mode for categorical ones aligns with best practices taught in class. This ensures that the imputation respects the nature of the data.\n",
    "\n",
    "- **Feature Segregation:**\n",
    "The dataset was first divided into numeric, ordinal, and nominal features. This classification guided which imputation strategy to use for each group, ensuring that the replacement value is appropriate for the type of data.\n",
    "\n",
    "- **Data Integrity:**\n",
    "By selecting these methods, we aim to minimise the distortion of the original data distribution, which is crucial for maintaining the quality and reliability of any subsequent analysis or model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "- Print `value_counts()` of the 'SEX' column and add a dummy variable named 'SEX_FEMALE' to `df` using `get_dummies()` (3 marks)\n",
    "- Carefully explain what the values of the new variable 'SEX_FEMALE' mean (2 mark)\n",
    "- Make sure the variable 'SEX' is deleted from the original dataframe   \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEX\n",
      "2.0    6162\n",
      "1.0    3838\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   EDUCATION                   10000 non-null  float64\n",
      " 2   MARRIAGE                    10000 non-null  float64\n",
      " 3   AGE                         10000 non-null  float64\n",
      " 4   PAY_0                       10000 non-null  float64\n",
      " 5   PAY_2                       10000 non-null  float64\n",
      " 6   PAY_3                       10000 non-null  float64\n",
      " 7   PAY_4                       10000 non-null  float64\n",
      " 8   PAY_5                       10000 non-null  float64\n",
      " 9   PAY_6                       10000 non-null  float64\n",
      " 10  BILL_AMT1                   10000 non-null  float64\n",
      " 11  BILL_AMT2                   10000 non-null  float64\n",
      " 12  BILL_AMT3                   10000 non-null  float64\n",
      " 13  BILL_AMT4                   10000 non-null  float64\n",
      " 14  BILL_AMT5                   10000 non-null  float64\n",
      " 15  BILL_AMT6                   10000 non-null  float64\n",
      " 16  PAY_AMT1                    10000 non-null  float64\n",
      " 17  PAY_AMT2                    10000 non-null  float64\n",
      " 18  PAY_AMT3                    10000 non-null  float64\n",
      " 19  PAY_AMT4                    10000 non-null  float64\n",
      " 20  PAY_AMT5                    10000 non-null  float64\n",
      " 21  PAY_AMT6                    10000 non-null  float64\n",
      " 22  default payment next month  10000 non-null  int64  \n",
      " 23  SEX_FEMALE                  10000 non-null  bool   \n",
      "dtypes: bool(1), float64(21), int64(2)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Print the counts of each value in the 'SEX' column\n",
    "print(df['SEX'].value_counts())\n",
    "\n",
    "# Create dummy variables for the 'SEX' column using get_dummies()\n",
    "sex_dummies = pd.get_dummies(df['SEX'], prefix='SEX')\n",
    "\n",
    "# In the dataset, 'SEX' is coded as:\n",
    "#   1 = Male, 2 = Female\n",
    "# The dummy variable for females is originally named 'SEX_2.0'. So I renamed it to 'SEX_FEMALE' as instructed\n",
    "sex_dummies.rename(columns={'SEX_2.0': 'SEX_FEMALE'}, inplace=True)\n",
    "\n",
    "# Add the 'SEX_FEMALE' column to the dataframe\n",
    "df = pd.concat([df, sex_dummies['SEX_FEMALE']], axis=1)\n",
    "\n",
    "# Remove the original 'SEX' column\n",
    "df.drop(columns=['SEX'], inplace=True)\n",
    "\n",
    "# To check 'SEX' column has been deleted\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**'SEX_FEMALE' Variable Explanation:**\n",
    "\n",
    "The new variable 'SEX_FEMALE' is a binary indicator derived from the original 'SEX' column using dummy encoding. Here are what the values represent:\n",
    "\n",
    "- 1: Indicates that the individual is female.\n",
    "\n",
    "- 0: Indicates that the individual is not female (i.e., the individual is male).\n",
    "\n",
    "This transformation simplifies the gender information into a clear and easy-to-use format for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. Print `value_counts()` of the 'MARRIAGE' column and *carefully* comment on what you notice in relation to the definition of this variable. \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARRIAGE\n",
      "2.0    5518\n",
      "1.0    4380\n",
      "3.0      82\n",
      "0.0      20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Print value counts for the 'MARRIAGE' column\n",
    "print(df['MARRIAGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Expected Categories:**\n",
    "\n",
    "According to the variable definition, **MARRIAGE** should have:\n",
    "\n",
    "- 1: Married\n",
    "\n",
    "- 2: Single\n",
    "\n",
    "- 3: Others\n",
    "\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- The majority of records are coded as **2.0 (Single)** and **1.0 (Married)**, which is in line with the expected values.\n",
    "\n",
    "- A small number of records (82) are classified as **3.0 (Others).**\n",
    "\n",
    "- **Unexpected Category:** There are **20 records** with a value of **0.0**, which does not match the defined categories.\n",
    "\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "The presence of a **0.0** value suggests that there might be data entry errors, misclassifications, or perhaps an undocumented category (such as unknown or missing) in the dataset. This anomaly should be investigated further to determine if these values need correction or exclusion during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q4**. \n",
    "\n",
    "- Apply `get_dummies()` to 'MARRIAGE' feature and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`. (5 marks)   \n",
    "- *Carefully consider* how to allocate all the values of 'MARRIAGE' across these 3 newly created features (5 marks)\n",
    "    - Do not delete observations \n",
    "    - Do not assume that the anomaly are missing observations      \n",
    "    - Explain what decision you made\n",
    "- Make sure that 'MARRIAGE' is deleted from `df`   \n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   EDUCATION                   10000 non-null  float64\n",
      " 2   AGE                         10000 non-null  float64\n",
      " 3   PAY_0                       10000 non-null  float64\n",
      " 4   PAY_2                       10000 non-null  float64\n",
      " 5   PAY_3                       10000 non-null  float64\n",
      " 6   PAY_4                       10000 non-null  float64\n",
      " 7   PAY_5                       10000 non-null  float64\n",
      " 8   PAY_6                       10000 non-null  float64\n",
      " 9   BILL_AMT1                   10000 non-null  float64\n",
      " 10  BILL_AMT2                   10000 non-null  float64\n",
      " 11  BILL_AMT3                   10000 non-null  float64\n",
      " 12  BILL_AMT4                   10000 non-null  float64\n",
      " 13  BILL_AMT5                   10000 non-null  float64\n",
      " 14  BILL_AMT6                   10000 non-null  float64\n",
      " 15  PAY_AMT1                    10000 non-null  float64\n",
      " 16  PAY_AMT2                    10000 non-null  float64\n",
      " 17  PAY_AMT3                    10000 non-null  float64\n",
      " 18  PAY_AMT4                    10000 non-null  float64\n",
      " 19  PAY_AMT5                    10000 non-null  float64\n",
      " 20  PAY_AMT6                    10000 non-null  float64\n",
      " 21  default payment next month  10000 non-null  int64  \n",
      " 22  SEX_FEMALE                  10000 non-null  bool   \n",
      " 23  MARRIAGE_MARRIED            10000 non-null  bool   \n",
      " 24  MARRIAGE_SINGLE             10000 non-null  bool   \n",
      " 25  MARRIAGE_OTHER              10000 non-null  bool   \n",
      "dtypes: bool(4), float64(20), int64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Create dummy variables for the 'MARRIAGE' feature using get_dummies()\n",
    "marriage_dummies = pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE')\n",
    "\n",
    "# Map the dummy columns to the target variable names:\n",
    "# According to the definition:\n",
    "#   1.0 -> Married  -> MARRIAGE_MARRIED\n",
    "#   2.0 -> Single   -> MARRIAGE_SINGLE\n",
    "#   3.0 -> Others   -> MARRIAGE_OTHER\n",
    "\n",
    "# However, the dataset also contains a value 0.0. \n",
    "# So I decided to allocate the 0.0 values to the \"Others\" category,\n",
    "# since they do not clearly fit into \"Married\" (1.0) or \"Single\" (2.0).\n",
    "\n",
    "df['MARRIAGE_MARRIED'] = marriage_dummies.get('MARRIAGE_1.0', 0)\n",
    "df['MARRIAGE_SINGLE']  = marriage_dummies.get('MARRIAGE_2.0', 0)\n",
    "df['MARRIAGE_OTHER']   = marriage_dummies.get('MARRIAGE_3.0', 0) + marriage_dummies.get('MARRIAGE_0.0', 0)\n",
    "\n",
    "# Remove the original 'MARRIAGE' column from the dataframe\n",
    "df.drop(columns=['MARRIAGE'], inplace=True)\n",
    "\n",
    "# To check 'MARRIAGE' column has been deleted\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**1. Creating Dummy Variables:**\n",
    "I used pd.get_dummies() to transform the 'MARRIAGE' column into dummy variables. This creates separate columns for each unique value (MARRIAGE_0.0, MARRIAGE_1.0, MARRIAGE_2.0, and MARRIAGE_3.0).\n",
    "\n",
    "**2. Mapping to Desired Dummy Names:**\n",
    "\n",
    "- MARRIAGE_MARRIED: I mapped observations with a value of 1.0 to this dummy variable.\n",
    "\n",
    "- MARRIAGE_SINGLE: I mapped observations with a value of 2.0 to this dummy variable.\n",
    "\n",
    "- MARRIAGE_OTHER: I mapped observations with a value of 3.0 to this variable.\n",
    "\n",
    "\n",
    "**Handling the Anomaly (0.0):**\n",
    "The dataset also contains 0.0, which is not part of the original definition. Instead of discarding or treating these as missing, I chose to add them into \"Others\" category. This decision is based on the reasoning that 0.0 does not clearly indicate either Married or Single, so it is most appropriately grouped under \"Others.\"\n",
    "\n",
    "\n",
    "**3. Finalising the DataFrame:**\n",
    "After creating and mapping the dummy variables, I removed the original 'MARRIAGE' column from the dataframe. This ensures our dataframe now only contains the new three clearly defined dummy variables while retaining all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q5**. In the column 'EDUCATION', convert the values {0, 5, 6} to the value 4. \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Replace values 0, 5, and 6 in the 'EDUCATION' column with 4\n",
    "df['EDUCATION'] = df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 3** Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Create a numpy array `y` from the first 7,000 observations of `default payment next month` column from `df` (2.5 marks)   \n",
    "- Create a numpy array `X`  from the first 7,000 observations of all the remaining variables in `df` (2.5 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "import numpy as np\n",
    "\n",
    "# Create the target array y from the first 7000 observations of the target variable\n",
    "y = df['default payment next month'].values[:7000]\n",
    "\n",
    "# Create the feature array X from the first 7000 observations of all remaining variables\n",
    "X = df.drop(columns=['default payment next month']).values[:7000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "\n",
    "- Use an appropriate `sklearn` library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets (2.5 marks) \n",
    "    - Set random_state to 31 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data to mean zero and variance one using an approapriate `sklearn` library (2.5 marks)   \n",
    "\n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into training and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=31, stratify=y\n",
    ")\n",
    "\n",
    "# Standardise the features: mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform on training data\n",
    "X_test_scaled = scaler.transform(X_test)        # Transform test data using the same scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 4**. Training Models and Interpretation (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Train one linear classifier we studied in class using standardised data (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 82.06%\n",
      "Test Accuracy: 80.52%\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialise the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=31, max_iter=1000)\n",
    "\n",
    "# Train the model using the standardised training data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = log_reg.predict(X_train_scaled)\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Compute accuracies for training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracy * 100))\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2.**\n",
    "\n",
    "- Train one nonlinear classifier we studied in class on the same dataset (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 99.98%\n",
      "Random Forest Test Accuracy: 81.05%\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialise the RandomForestClassifier with a fixed random state for reproducibility\n",
    "rf_clf = RandomForestClassifier(random_state=31)\n",
    "\n",
    "# Train the model using the standardised training data\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both the training and test datasets\n",
    "y_train_pred_rf = rf_clf.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy for the training and test datasets\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "# Print the results\n",
    "print(\"Random Forest Training Accuracy: {:.2f}%\".format(train_accuracy_rf * 100))\n",
    "print(\"Random Forest Test Accuracy: {:.2f}%\".format(test_accuracy_rf * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. \n",
    "\n",
    "- Comment on the accuracy results obtained from the two classifiers (6 marks)\n",
    "- Based on our investigation into credit card default predictions in this assignment, which model would you recommend? (4 marks)\n",
    "\n",
    "\n",
    "(Total: 10 marks)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "\n",
    "**Comments on Accuracy Results**\n",
    "\n",
    "**1. Logistic Regression (Linear Classifier):**\n",
    "\n",
    "- Training Accuracy: 82.06%\n",
    "\n",
    "- Test Accuracy: 80.52%\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The logistic regression model shows consistent performance between the training and test sets. The small difference in accuracy (about 1.5 percentage points) indicates that the model generalises well and is not overfitting.\n",
    "\n",
    "**2. Random Forest (Non-Linear Classifier):**\n",
    "\n",
    "- Training Accuracy: 99.98%\n",
    "\n",
    "- Test Accuracy: 81.05%\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "Although the Random Forest model achieves nearly perfect accuracy on the training data, its test accuracy is very similar to the logistic regression results. This large gap between training and test accuracy suggests that the Random Forest model is overfitting the training data by memorising the training examples rather than learning generalisable patterns.\n",
    "\n",
    "**3. Model Recommendation**\n",
    "\n",
    "Based on my investigation into credit card default predictions, I would recommend using the Logistic Regression model.\n",
    "\n",
    "**Reasons:**\n",
    "\n",
    "- **Generalisation:** Logistic Regression provides a more reliable and robust performance on unseen data, with similar test accuracy to the Random Forest but without the risk of overfitting.\n",
    "\n",
    "- **Interpretability:** The linear nature of logistic regression makes it easier to interpret and explain, which is valuable in a business analytics context.\n",
    "\n",
    "- **Simplicity:** A simpler model that performs comparably on the test set is preferable, especially when it avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Criteria\n",
    "\n",
    "To achieve a perfect score, your solutions must adhere to the criteria outlined below:\n",
    "\n",
    "- Ensure that all numerical answers are accurate.\n",
    "- Utilize the exact Python functions and libraries specified within the assignment instructions.\n",
    "- For any written responses, provide accurate information, articulated in clear, complete sentences.\n",
    "- Do not add extra cells beyond what is provided in the notebook.\n",
    "- Do not print output with your code unless explicitly instructed to do so.\n",
    "- Maintain a clean and organised notebook layout that is easy to follow.\n",
    "- Marks will be deducted for not following the above instructions.\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
